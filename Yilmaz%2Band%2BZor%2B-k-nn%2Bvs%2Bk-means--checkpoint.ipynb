{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Machine learning is a method for making inferences from the existing data by the statistical and mathematical ways. K-Means and K-Nearest Neighbors (K-NN) are two commonly used clustering algorithms in machine learning. In this project, the working principle of two algorithms is explained. The Iris and Wine data sets were chosen to implement the algorithms. For both of the data sets, k-means and k-NN were applied and obtained results were compared. According to these results, the algorithm which makes more accurate estimates for the selected data set is interpreted as preferable. The Python program was used in order to apply the algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "The Comparison of K-Means and K-Nearest Neighbors (k-NN) Clustering  Algorithms \n",
    "\n",
    "## Members\n",
    "\n",
    "1. First member: Kübra Zeynep Zor  [`zork@itu.edu.tr`] (k-means algorithm)\n",
    "2. Second member: Nur Yılmaz [`yilmaznur@itu.edu.tr`] (k-NN algorithm)\n",
    "\n",
    "## Description of the project\n",
    "To create a statistical model using these two data sets: Iris data set and Wine data set. Also, to make assumptions by applying  k-means and k-NN clustering algorithms. We will use scikit-learn library of Python.\n",
    "\n",
    "### The methods to be used\n",
    "\n",
    "**k-Means**: K-Means is the one of the earliest clustering method. Although it was an idea suggested by Hugo Steinhaus in 1957, it was developed by J.B. MacQueen in 1967. The algorithm based on dividing n observations into k disjoint sets. Similar elements are in the same cluster. So, every element should belong to only one cluster. The algorithm aims to reduce the sum of the distance of each point to the k center of the circle. At first cluster centers are identified. Then, the distance of each sample from the selected centers is calculated. The samples are put into the closest samples of k clusters. After that, the new center values of clusters are assigned as averages of the samples. These steps are repeated until the center points do not change. \n",
    "Eucledian distance can be used to calculate the distance of the points to the cluster centers. The Euclidean distance is:$$p=(p_1,p_2,...,p_n)$$ $$and$$ $$q=(q_1,q_2,...,q_n)$$ $$ \\sqrt{\\sum_{i=1}^{n}(p_i-q_i)^2}=\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^2}$$\n",
    "\n",
    "Algorithmic steps for k-means clustering:\n",
    "\n",
    "Let $X=(x_1,x_2,...,x_n)$ be set of data points. Then, and $V=(v_1,v_2,...v_c)$ be the set of centers.\n",
    "\n",
    "1. Randomly place 'c' cluster centers.\n",
    "2. Calculate the distance from each data point $x_i$ to the cluster centers.\n",
    "3. Find nearest cluster centers and assign the data point this cluster.\n",
    "4. Recalculate the new cluster center using:$$v_i=\\frac{1}{c_i}\\sum_{j=1}^{c_i}x_i$$ where '$c_i$ ' represents the number of data points ith cluster\n",
    "5. Recalculate the distance between each data point and new cluster centers.\n",
    "6. Stop if the location of the data points does not change, otherwise go back to step 3.\n",
    "\n",
    "This algorithm aims an minimizing an objective function known as squared error function given by: $$J(V)=\\sum_{i=1}^{c}\\sum_{j=1}^{c_i}(||x_i-v_j||)^2$$ where, $||x_i-v_j||$ is the Euclidean distance $x_i$ and $v_j$,\n",
    "'$c_i$' is the number of data points ith cluster,\n",
    "'c' is the number of cluster centers\n",
    "\n",
    "K-Means Advantages\n",
    "\n",
    "1. If there are many variables, the k-means gives faster results than the hierarchical clustering. (If k is small)\n",
    "2. K-Means produces tighter clusters than hierarchical clustering, especially if the clusters are globular.\n",
    "3. The data set must be well-separated for the best result of the algorithm.\n",
    "\n",
    "K-Means Disadvantages\n",
    "\n",
    "1. It is difficult to compare the quality of the final clusters because the results may change according to different initial partitions or k value.\n",
    "2. To determine the k value can be difficult.\n",
    "3. If the clusters are non-globular the algorithm does not work well.\n",
    "4. Computationally difficult.\n",
    "\n",
    "\n",
    "\n",
    "**k-NN**: The nearest neighbors algorithm which solves the classification problem is an instance-based learning method. It is one of the simplest method among machine learning algorithms. The general logic of the algorithm is the investigated sample belongs to the nearest cluster. The choice of k is also important. Generally large values of k reduce the effect of noise on the classification, but decrease the clarity of boundaries between classes. A useful k can be selected by some techniques such as bootstrap method. The use of all of the observations in the data sets for estimation leads to time loss and spoilage. Therefore, the samples that represent the population well are needed. For this process, the data is resampled by displacement depending on the chance using observations in the data set of any size. In this way, new data sets of various amounts and sizes are created. Thus, the algorithms are applied to the new data sets that are formed. This method was developed by Bradley Efron in 1979 and is called the Bootstrap (Resampling) method.\n",
    "\n",
    "The steps of the algorithm are as follows:\n",
    "\n",
    "1. Decide how many of the nearest neighbors i.e. k will be looked and assign n known samples in order to classify the test sample.\n",
    "2. Calculate  the distances from the sample to the other n samples and sort these distances.\n",
    "3. Choose k nearest neighbors with minimum distance.\n",
    "4. Determine the most repetitive sample among the lined up distances.\n",
    "5. The test sample belongs to the class of the most repetitive sample.\n",
    "6. Stop the algorithm when all data points in the sample have been classified.\n",
    "\n",
    "The distance between samples can be measured using metrics such as L1 (first norm) Manhattan -taxicab- distance or L2 (second norm) Euclidean distance. The Euclidean distance may not be effective when the size of data increases so we will use Manhattan distance. The Manhattan name is based on the gridlike street shape in Manhattan borough of New York. It is also called the taxicab norm which is the distance wanted to take passengers with a car in the city where square blocks are arranged.\n",
    "\n",
    "The taxicab distances $(d_1)$ are calculated using: $$d_1(p,q) = ||p-q||_1 = \\sum_{i=1}^{n}|p_i-q_i|$$ where $(p_1,p_2,...,p_n)$ and $q=(q_1,q_2,...,q_n)$ are vectors.\n",
    "\n",
    "Advantages of k-NN\n",
    "\n",
    "1. It is used not only quantitative but also qualitative data sets.\n",
    "2. If the number of samples is large enough, it usually gives a good classification result.\n",
    "3. The algorithm is resistant to noisy data (any data that has been stored, or changed in such a way that cannot be read or used by the program that originally created)\n",
    "\n",
    "Disadvantages of k-NN\n",
    "\n",
    "1. Determining the value of k may sometimes be difficult.\n",
    "2. Computation cost is pretty high because it is needed to calculate the distances of each test samples to each training samples.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### The data\n",
    "\n",
    "The Iris Flower data set or Fisher’s Iris data set is a multivariate data set which was explained by statistician and biologist Ronald Fisher. The dataset was created by taking 50 samples from three species: Iris setosa, Iris virginica and Iris versicolor. Four attributes were analyzed from each sample: the length and the width of the sepals and petals, in centimetres. Fisher improved a linear discriminant model in order to classificate the species according to these features.\n",
    "\n",
    "Data size: 150 entries\n",
    "\n",
    "Data distribution: 50 entries for each class\n",
    "\n",
    "It has 5 columns:\n",
    "\n",
    "1. sepal length in cm (in the range of 4.3 and 7.9)\n",
    "2. sepal width in cm (in the range of 2.0 and 4.4)\n",
    "3. petal length in cm (in the range of 1.0 and 6.9)\n",
    "4. petal width in cm (in the range of 0.1 and 2.5)\n",
    "5. 3 classes:\n",
    "\n",
    "   1) Iris Setosa \n",
    "   \n",
    "   2) Iris Versicolour \n",
    "   \n",
    "   3) Iris Virginica\n",
    "\n",
    "\n",
    "The Wine data set  expresses the chemical analysis of three kinds of wine which are produced in Italy. This analysis shows the amount of the same 13 ingredients in each wine.\n",
    "\n",
    "Data size: 178 entries\n",
    "\n",
    "Data distribution: 59, 71, and 48 entries for each class\n",
    "\n",
    "It has 14 columns (First attribute is the class identifier and 13 ingredients are the remaining part): \n",
    "\n",
    "1. 3 classes (1,2,3)\n",
    "2. Alcohol (in the range of 11.03 and 14.83)\n",
    "3. Malic acid (in the range of 0.74 and 5.8)\n",
    "4. Ash (in the range of 1.36 and 3.23)\n",
    "5. Alcalinity of ash (in the range of 10.6 and 28.5)\n",
    "6. Magnesium (in the range of 84 and 128)\n",
    "7. Total phenols (in the range of 1.1 and 3.88) \n",
    "8. Flavanoids (in the range of 0.8 and 3.93)\n",
    "9. Nonflavanoid phenols (in the range of 0.13 and 0.66)\n",
    "10. Proanthocyanins (in the range of 1.1 and 3.58)\n",
    "11. Color intensity (in the range of 3.05 and 8.90)\n",
    "12. Hue (in the range of 0.48 and 1.71)\n",
    "13. OD280/OD315 of diluted wines (in the range of 1.27 and 3.92)\n",
    "14. Proline (in the range of 278 and 1480)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1.  http://acikerisim.ticaret.edu.tr:8080/xmlui/bitstream/handle/11467/208/M00041.pdf?sequence=1&isAllowed=y\n",
    "2.  http://dergipark.ulakbim.gov.tr/egeziraat/article/viewFile/5000154071/5000139378\n",
    "3.  http://www.emo.org.tr/ekler/8c1874c96244659_ek.pdf\n",
    "4.  https://en.wikipedia.org/wiki/K-means_clustering\n",
    "5.  https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "6.  https://en.wikipedia.org/wiki/Taxicab_geometry\n",
    "7.  http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/K-Means_Clustering_Overview.htm\n",
    "8.  http://mirlab.org/jang/books/dcpr/dataSetIris.asp?title=2-2%20Iris%20Dataset\n",
    "9.  http://mirlab.org/jang/books/dcpr/dataSetWine.asp?title=2-3%20Wine%20Dataset\n",
    "10. http://people.revoledu.com/kardi/tutorial/KNN/Strength%20and%20Weakness.htm\n",
    "11. www.safagurcan.org/doktora/sbekikare.ppt\n",
    "12. http://searchbusinessanalytics.techtarget.com/definition/noisy-data\n",
    "13. https://sites.google.com/site/dataclusteringalgorithms/k-means-clustering-algorithm\n",
    "14. http://stackoverflow.com/questions/10665889/how-to-take-column-slices-of-dataframe-in-pandas\n",
    "15. https://www.youtube.com/watch?v=hd1W4CyPX58\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cluster, datasets\n",
    "import sklearn.metrics as sm\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import chi2\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means algorithm for Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,random_state=0, test_size=0.30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris data set was loaded in the first line of the above cell. In the second line, the data is divided into two parts: test and train, with 30% for the test. It is used the train_test_split function of scikit-learn library for this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_means_iris= cluster.KMeans(n_clusters=3)\n",
    "k_means_iris.fit(X_train,y_train)\n",
    "y_pred=k_means_iris.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the model is set up with k-means clustering algorithm by using fit method for X input and y output training data set. The output for the test data is obtained by predict method from model in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 16]\n",
      " [18  0  0]\n",
      " [ 4  7  0]]\n"
     ]
    }
   ],
   "source": [
    "iris_kmeans=sm.confusion_matrix(y_test,y_pred)\n",
    "print(iris_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is used to measure the accuracy of the algorithm. The rows of the matrix corresponds to the actual values and the columns corresponds to the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68.429752066115697,\n",
       " 4.8684902402509469e-14,\n",
       " 4,\n",
       " array([[ 7.82222222,  2.48888889,  5.68888889],\n",
       "        [ 8.8       ,  2.8       ,  6.4       ],\n",
       "        [ 5.37777778,  1.71111111,  3.91111111]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.chi2_contingency(iris_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi square test is applied to the entries which are obtained from the confusion matrix. Chi square test is based on whether the difference between observed and expected frequencies is meaningful. It is used in the analysis of qualitatively stated data. The situations which are used:\n",
    "\n",
    "1. Determining whether there is a difference between groups\n",
    "2. Determining whether there is a link between two variables\n",
    "3. For group-to-group homogeneity test\n",
    "4. To determine whether the distribution from the sample matches any statistical distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# k-NN algorithm for Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps applied for the k means algorithm are adapted to the k nearest neighbors algorithm in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, test_size=0.40, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_iris = KNeighborsClassifier()\n",
    "knn_iris.fit(X_train2, y_train2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
    "           weights='uniform')\n",
    "y_pred2=knn_iris.predict(X_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 22  1]\n",
      " [ 0  2 19]]\n"
     ]
    }
   ],
   "source": [
    "iris_knn=confusion_matrix(y_test2,y_pred2)\n",
    "print(iris_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104.78674948240167,\n",
       " 9.4044593377724569e-22,\n",
       " 4,\n",
       " array([[ 4.26666667,  6.4       ,  5.33333333],\n",
       "        [ 6.13333333,  9.2       ,  7.66666667],\n",
       "        [ 5.6       ,  8.4       ,  7.        ]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.chi2_contingency(iris_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# k-means algorithm for Wine data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wine= pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wine data set is read from web because it is not in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(df_wine.iloc[:,[2,3,4,5,6,7,8,9,10,11,12,13]], df_wine.iloc[:,[0]],random_state=0, test_size=0.30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is divided by the train_test_split function and the required columns for data and target sections are selected with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_means_wine= cluster.KMeans(n_clusters=3)\n",
    "k_means_wine.fit(X_train3,y_train3)\n",
    "y_pred3=k_means_wine.predict(X_test3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 5  0 14  0]\n",
      " [ 5 17  0  0]\n",
      " [ 8  5  0  0]]\n"
     ]
    }
   ],
   "source": [
    "wine_kmeans=sm.confusion_matrix(y_test3,y_pred3)\n",
    "print(wine_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means algorithm is applied to the Wine data set in the same way with the İris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.879245156757115,\n",
       " 4.2125624544131398e-09,\n",
       " 4,\n",
       " array([[ 6.33333333,  4.92592593,  7.74074074],\n",
       "        [ 7.33333333,  5.7037037 ,  8.96296296],\n",
       "        [ 4.33333333,  3.37037037,  5.2962963 ]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.chi2_contingency([[5,14,0],[5,0,17],[8,0,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn algorithm for Wine data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps applied for the k-means algorithm are addapted to the k-nearest neighbors algorithm in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(df_wine.iloc[:,[2,3,4,5,6,7,8,9,10,11,12,13]], df_wine.iloc[:,[0]],random_state=0, test_size=0.40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_wine = KNeighborsClassifier()\n",
    "knn_wine.fit(X_train4, y_train4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
    "           metric_params=None,n_jobs=1, n_neighbors=3, p=2,\n",
    "           weights='uniform')\n",
    "y_pred4=knn_wine.predict(X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  1  0]\n",
      " [ 2 21  8]\n",
      " [ 6  7  6]]\n"
     ]
    }
   ],
   "source": [
    "wine_knn=confusion_matrix(y_test4,y_pred4)\n",
    "print(wine_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.989427760928258,\n",
       " 3.996134069192527e-09,\n",
       " 4,\n",
       " array([[  8.86111111,   8.86111111,   4.27777778],\n",
       "        [ 12.48611111,  12.48611111,   6.02777778],\n",
       "        [  7.65277778,   7.65277778,   3.69444444]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.chi2_contingency([[21,1,0],[2,21,8],[6,7,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is compared the accuracy of the k-means and k-nearest neighbors algorithms for Iris data set k-NN algorithm is preferable. Since chi square test gives the bigger result for k-NN. \n",
    "\n",
    "k-means test result: 68.429752066115697\n",
    "\n",
    "k-NN    test result: 104.78674948240167\n",
    "\n",
    "If it is compared the accuracy of the k-means and k-nearest neighbors algorithms for Wine data set, since the values are very close to each other, it can be said that both algorithms gave the same result for Wine data set.\n",
    "\n",
    "\n",
    "k-means test result: 44.879245156757115\n",
    "\n",
    "k-NN    test result: 44.989427760928258\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study is made up of subjects that we are not familiar with and we do not have enough information. For this reason, at first we had difficulties in perceiving the issues. Later, when we do researches and went into practice, it became more meaningful for us. We also had the opportunity to conduct both theoretical and practical work for the first time. Moreover, by learning these algorithms, we have an idea about machine learning techniques. Finally, we use the Jupyter Notebook platform where both code and text can be written, so we do not have any difficulty in writing our project.\n",
    "\n",
    "If we come to the application part, we used code and algorithms from many sources with trial and error, and finally we reached the right conclusion. For example, we read the data from web using the pandas dataframe because the wine data set is not available in the scikit-learn library. Even though we tried to use numpy array for this procedure at the beginning, we prefered pandas later. Because we could not run our codes correctly for the algorithms when we use numpy. Besides, we can say that the implementation of two algorithms is equivalent for convenience. Since we used similar codes to split the data sets, set up the model, and test the results from the methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
