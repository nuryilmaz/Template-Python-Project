{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "The comparison of k-means and k-nearest neighbors (k-NN) clustering  algorithms \n",
    "\n",
    "## Members\n",
    "\n",
    "1. First member: Kübra Zeynep Zor  [`zork@itu.edu.tr`]\n",
    "2. Second member: Nur Yılmaz [`yilmaznur@itu.edu.tr`]\n",
    "\n",
    "## Description of the project\n",
    "To create a statistical model using these two data sets: Iris data set and Wine data set. Also, to make assumptions by applying  K-means and K-NN clustering algorithms. We will use scikit-learn library of Python.\n",
    "\n",
    "### The methods to be used\n",
    "\n",
    "**k-means**: The algorithm based on  dividing n observations  into k disjoint sets. Similar elements are in the same cluster. So every element should belong to only one cluster.The algorithm aims to reduce the sum of the distance of each point to the k center of the circle.At first cluster centers are identified.Then the distance of each sample from the selected centers is calculated.The samples are put in to the closest samples of k clusters.After that, the new center values of clusters are assigned as averages of the samples.These steps are repeated until the center points do not change. \n",
    "Eucledian distance can be used to calculate the distance of the points to the cluster centers. The Euclidean distance is:$$p=(p_1,p_2,...,p_n)$$ $$and$$ $$q=(q_1,q_2,...,q_n)$$ $$ \\sqrt{\\sum_{i=1}^{n}(p_i-q_i)^2}=\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^2}$$\n",
    "\n",
    "Algorithmic steps for k-means clustering:\n",
    "\n",
    "Let $X=(x_1,x_2,...,x_n)$ be set of data points. Then, and $V=(v_1,v_2,...v_c)$ be the set of centers.\n",
    "\n",
    "1. Randomly place 'c' cluster centers.\n",
    "2. Calculate the distance from each data point $x_i$ to the cluster centers.\n",
    "3. Find nearest cluster centers and assign the data point this cluster.\n",
    "4. Recalculate the new cluster center using:$$v_i=\\frac{1}{c_i}\\sum_{j=1}^{c_i}x_i$$ where '$c_i$ ' represents the number of data points ith cluster\n",
    "5. Recalculate the distance between each data point and new cluster centers.\n",
    "6. Stop if the location of the data points does not change, otherwise go back to step 3.\n",
    "\n",
    "This algorithm aims an minimizing an objective function know as squared error function given by: $$J(V)=\\sum_{i=1}^{c}\\sum_{j=1}^{c_i}(||x_i-v_j||)^2$$ where, $||x_i-v_j||$ is the Euclidean distance $x_i$ and $v_j$,\n",
    "'$c_i$' is the number of data points ith cluster,\n",
    "'c' is the number of cluster centers\n",
    "\n",
    "k-means algorithm is fast and understandable. It gives the best result when the data set is well seperated from each other. It is necessary to determine appropriate number of clusters before starting the algorithm. Another disadvantage is that a high value can significantly change the average of cluster and cluster center. The algorithm is not sensitive to noisy data. Algorithm can not be applied for non-linear data set.\n",
    "\n",
    "**k-NN**: The nearest neighbors algorithm that solves the classification problem is an instance-based learning method.It is one of the simplest of all machine learning algorithms.The general logic of the algorithm is the investigated sample belongs to the nearest cluster.The choice of k is also important.Generally large values of k reduce the effect of noise on the classification, but decrease the clarity of boundaries between classes.A useful k can be selected by some techniques such as bootstrap method.\n",
    "\n",
    "The steps of the algorithm are as follows:\n",
    "\n",
    "1. Decide how many of the nearest neighbors i.e. k will be looked.\n",
    "2. Choose n known samples in order to classify the test sample.\n",
    "3. Calculate  the distances from the sample to the other samples and choose k nearest samples.\n",
    "4. Determine the most repetitive sample.\n",
    "5. The test sample belongs to the class which is the most repetitive sample.\n",
    "\n",
    "The distance between samples can be measured using metrics as L1 (first norm) Manhattan -taxicab- distance or L2 (second norm) Euclidean distance.The Euclidean distance may not be effective when data size increases so we will use Manhattan distance.The Manhattan name based on the gridlike street shape in Manhattan borough of New York.In fact, the taxicab norm is the distance wanted to take passengers with a car in the city where square blocks are arranged.\n",
    "\n",
    "The taxicab distances $(d_1)$ are calculated using: $$d_1(p,q) = ||p-q||_1 = \\sum_{i=1}^{n}|p_i-q_i|$$ where $(p,q)$ are vectors $p=(p_1,p_2,...,p_n)$ and $q=(q_1,q_2,...,q_n)$.\n",
    "\n",
    "\n",
    " \n",
    "The algorithm is resistant to noisy datas but is not much effective in multidimentional data sets.It is used not only quantitative but also qualitative data sets.Some of the data sets the algoritm applies to are Breast Cancer Wisconsin, Cardiotocography, Leaf etc.\n",
    "\n",
    "### The data\n",
    "\n",
    "The Iris Flower data set or Fisher’s Iris data set is a multivariate data set which was explained by  statistician and biologist Ronald Fisher.The dataset was created by taking 50 samples from three species :Iris setosa, Iris virginica and Iris versicolor. Four attributes were analyzed from each sample: the length and the width of the sepals and petals, in centimetres. Fisher improved a linear discriminant model in order to classificate the species according to these features.\n",
    "\n",
    "Data size: 150 entries\n",
    "Data distribution: 50 entries for each class\n",
    "\n",
    "\n",
    "The Wine data set  expresses the chemical analysis of three wine kinds produced in Italy. This analysis shows the amount of same 13 ingredients in each wine.\n",
    "\n",
    "Data size: 178 entries\n",
    "3 classes\n",
    "Data distribution: 59, 71, and 48 entries for each class\n",
    "\n",
    "### References\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/K-means_clustering\n",
    "2. https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "3. http://mirlab.org/jang/books/dcpr/dataSetIris.asp?title=2-2%20Iris%20Dataset\n",
    "4. http://mirlab.org/jang/books/dcpr/dataSetWine.asp?title=2-3%20Wine%20Dataset\n",
    "5. http://www.emo.org.tr/ekler/8c1874c96244659_ek.pdf\n",
    "6. https://www.youtube.com/watch?v=hd1W4CyPX58\n",
    "7. https://en.wikipedia.org/wiki/Taxicab_geometry\n",
    "8. https://sites.google.com/site/dataclusteringalgorithms/k-means-clustering-algorithm\n",
    "9. http://acikerisim.ticaret.edu.tr:8080/xmlui/bitstream/handle/11467/208/M00041.pdf?sequence=1&isAllowed=y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'Iris-setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['5.4', '3.9', '1.7', '0.4', 'Iris-setosa'],\n",
       " ['4.6', '3.4', '1.4', '0.3', 'Iris-setosa'],\n",
       " ['5.0', '3.4', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['4.4', '2.9', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'],\n",
       " ['5.4', '3.7', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['4.8', '3.4', '1.6', '0.2', 'Iris-setosa'],\n",
       " ['4.8', '3.0', '1.4', '0.1', 'Iris-setosa'],\n",
       " ['4.3', '3.0', '1.1', '0.1', 'Iris-setosa'],\n",
       " ['5.8', '4.0', '1.2', '0.2', 'Iris-setosa'],\n",
       " ['5.7', '4.4', '1.5', '0.4', 'Iris-setosa'],\n",
       " ['5.4', '3.9', '1.3', '0.4', 'Iris-setosa'],\n",
       " ['5.1', '3.5', '1.4', '0.3', 'Iris-setosa'],\n",
       " ['5.7', '3.8', '1.7', '0.3', 'Iris-setosa'],\n",
       " ['5.1', '3.8', '1.5', '0.3', 'Iris-setosa'],\n",
       " ['5.4', '3.4', '1.7', '0.2', 'Iris-setosa'],\n",
       " ['5.1', '3.7', '1.5', '0.4', 'Iris-setosa'],\n",
       " ['4.6', '3.6', '1.0', '0.2', 'Iris-setosa'],\n",
       " ['5.1', '3.3', '1.7', '0.5', 'Iris-setosa'],\n",
       " ['4.8', '3.4', '1.9', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.0', '1.6', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.4', '1.6', '0.4', 'Iris-setosa'],\n",
       " ['5.2', '3.5', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['5.2', '3.4', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.7', '3.2', '1.6', '0.2', 'Iris-setosa'],\n",
       " ['4.8', '3.1', '1.6', '0.2', 'Iris-setosa'],\n",
       " ['5.4', '3.4', '1.5', '0.4', 'Iris-setosa'],\n",
       " ['5.2', '4.1', '1.5', '0.1', 'Iris-setosa'],\n",
       " ['5.5', '4.2', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'],\n",
       " ['5.0', '3.2', '1.2', '0.2', 'Iris-setosa'],\n",
       " ['5.5', '3.5', '1.3', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'],\n",
       " ['4.4', '3.0', '1.3', '0.2', 'Iris-setosa'],\n",
       " ['5.1', '3.4', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.5', '1.3', '0.3', 'Iris-setosa'],\n",
       " ['4.5', '2.3', '1.3', '0.3', 'Iris-setosa'],\n",
       " ['4.4', '3.2', '1.3', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.5', '1.6', '0.6', 'Iris-setosa'],\n",
       " ['5.1', '3.8', '1.9', '0.4', 'Iris-setosa'],\n",
       " ['4.8', '3.0', '1.4', '0.3', 'Iris-setosa'],\n",
       " ['5.1', '3.8', '1.6', '0.2', 'Iris-setosa'],\n",
       " ['4.6', '3.2', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['5.3', '3.7', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['5.0', '3.3', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['7.0', '3.2', '4.7', '1.4', 'Iris-versicolor'],\n",
       " ['6.4', '3.2', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['6.9', '3.1', '4.9', '1.5', 'Iris-versicolor'],\n",
       " ['5.5', '2.3', '4.0', '1.3', 'Iris-versicolor'],\n",
       " ['6.5', '2.8', '4.6', '1.5', 'Iris-versicolor'],\n",
       " ['5.7', '2.8', '4.5', '1.3', 'Iris-versicolor'],\n",
       " ['6.3', '3.3', '4.7', '1.6', 'Iris-versicolor'],\n",
       " ['4.9', '2.4', '3.3', '1.0', 'Iris-versicolor'],\n",
       " ['6.6', '2.9', '4.6', '1.3', 'Iris-versicolor'],\n",
       " ['5.2', '2.7', '3.9', '1.4', 'Iris-versicolor'],\n",
       " ['5.0', '2.0', '3.5', '1.0', 'Iris-versicolor'],\n",
       " ['5.9', '3.0', '4.2', '1.5', 'Iris-versicolor'],\n",
       " ['6.0', '2.2', '4.0', '1.0', 'Iris-versicolor'],\n",
       " ['6.1', '2.9', '4.7', '1.4', 'Iris-versicolor'],\n",
       " ['5.6', '2.9', '3.6', '1.3', 'Iris-versicolor'],\n",
       " ['6.7', '3.1', '4.4', '1.4', 'Iris-versicolor'],\n",
       " ['5.6', '3.0', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['5.8', '2.7', '4.1', '1.0', 'Iris-versicolor'],\n",
       " ['6.2', '2.2', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['5.6', '2.5', '3.9', '1.1', 'Iris-versicolor'],\n",
       " ['5.9', '3.2', '4.8', '1.8', 'Iris-versicolor'],\n",
       " ['6.1', '2.8', '4.0', '1.3', 'Iris-versicolor'],\n",
       " ['6.3', '2.5', '4.9', '1.5', 'Iris-versicolor'],\n",
       " ['6.1', '2.8', '4.7', '1.2', 'Iris-versicolor'],\n",
       " ['6.4', '2.9', '4.3', '1.3', 'Iris-versicolor'],\n",
       " ['6.6', '3.0', '4.4', '1.4', 'Iris-versicolor'],\n",
       " ['6.8', '2.8', '4.8', '1.4', 'Iris-versicolor'],\n",
       " ['6.7', '3.0', '5.0', '1.7', 'Iris-versicolor'],\n",
       " ['6.0', '2.9', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['5.7', '2.6', '3.5', '1.0', 'Iris-versicolor'],\n",
       " ['5.5', '2.4', '3.8', '1.1', 'Iris-versicolor'],\n",
       " ['5.5', '2.4', '3.7', '1.0', 'Iris-versicolor'],\n",
       " ['5.8', '2.7', '3.9', '1.2', 'Iris-versicolor'],\n",
       " ['6.0', '2.7', '5.1', '1.6', 'Iris-versicolor'],\n",
       " ['5.4', '3.0', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['6.0', '3.4', '4.5', '1.6', 'Iris-versicolor'],\n",
       " ['6.7', '3.1', '4.7', '1.5', 'Iris-versicolor'],\n",
       " ['6.3', '2.3', '4.4', '1.3', 'Iris-versicolor'],\n",
       " ['5.6', '3.0', '4.1', '1.3', 'Iris-versicolor'],\n",
       " ['5.5', '2.5', '4.0', '1.3', 'Iris-versicolor'],\n",
       " ['5.5', '2.6', '4.4', '1.2', 'Iris-versicolor'],\n",
       " ['6.1', '3.0', '4.6', '1.4', 'Iris-versicolor'],\n",
       " ['5.8', '2.6', '4.0', '1.2', 'Iris-versicolor'],\n",
       " ['5.0', '2.3', '3.3', '1.0', 'Iris-versicolor'],\n",
       " ['5.6', '2.7', '4.2', '1.3', 'Iris-versicolor'],\n",
       " ['5.7', '3.0', '4.2', '1.2', 'Iris-versicolor'],\n",
       " ['5.7', '2.9', '4.2', '1.3', 'Iris-versicolor'],\n",
       " ['6.2', '2.9', '4.3', '1.3', 'Iris-versicolor'],\n",
       " ['5.1', '2.5', '3.0', '1.1', 'Iris-versicolor'],\n",
       " ['5.7', '2.8', '4.1', '1.3', 'Iris-versicolor'],\n",
       " ['6.3', '3.3', '6.0', '2.5', 'Iris-virginica'],\n",
       " ['5.8', '2.7', '5.1', '1.9', 'Iris-virginica'],\n",
       " ['7.1', '3.0', '5.9', '2.1', 'Iris-virginica'],\n",
       " ['6.3', '2.9', '5.6', '1.8', 'Iris-virginica'],\n",
       " ['6.5', '3.0', '5.8', '2.2', 'Iris-virginica'],\n",
       " ['7.6', '3.0', '6.6', '2.1', 'Iris-virginica'],\n",
       " ['4.9', '2.5', '4.5', '1.7', 'Iris-virginica'],\n",
       " ['7.3', '2.9', '6.3', '1.8', 'Iris-virginica'],\n",
       " ['6.7', '2.5', '5.8', '1.8', 'Iris-virginica'],\n",
       " ['7.2', '3.6', '6.1', '2.5', 'Iris-virginica'],\n",
       " ['6.5', '3.2', '5.1', '2.0', 'Iris-virginica'],\n",
       " ['6.4', '2.7', '5.3', '1.9', 'Iris-virginica'],\n",
       " ['6.8', '3.0', '5.5', '2.1', 'Iris-virginica'],\n",
       " ['5.7', '2.5', '5.0', '2.0', 'Iris-virginica'],\n",
       " ['5.8', '2.8', '5.1', '2.4', 'Iris-virginica'],\n",
       " ['6.4', '3.2', '5.3', '2.3', 'Iris-virginica'],\n",
       " ['6.5', '3.0', '5.5', '1.8', 'Iris-virginica'],\n",
       " ['7.7', '3.8', '6.7', '2.2', 'Iris-virginica'],\n",
       " ['7.7', '2.6', '6.9', '2.3', 'Iris-virginica'],\n",
       " ['6.0', '2.2', '5.0', '1.5', 'Iris-virginica'],\n",
       " ['6.9', '3.2', '5.7', '2.3', 'Iris-virginica'],\n",
       " ['5.6', '2.8', '4.9', '2.0', 'Iris-virginica'],\n",
       " ['7.7', '2.8', '6.7', '2.0', 'Iris-virginica'],\n",
       " ['6.3', '2.7', '4.9', '1.8', 'Iris-virginica'],\n",
       " ['6.7', '3.3', '5.7', '2.1', 'Iris-virginica'],\n",
       " ['7.2', '3.2', '6.0', '1.8', 'Iris-virginica'],\n",
       " ['6.2', '2.8', '4.8', '1.8', 'Iris-virginica'],\n",
       " ['6.1', '3.0', '4.9', '1.8', 'Iris-virginica'],\n",
       " ['6.4', '2.8', '5.6', '2.1', 'Iris-virginica'],\n",
       " ['7.2', '3.0', '5.8', '1.6', 'Iris-virginica'],\n",
       " ['7.4', '2.8', '6.1', '1.9', 'Iris-virginica'],\n",
       " ['7.9', '3.8', '6.4', '2.0', 'Iris-virginica'],\n",
       " ['6.4', '2.8', '5.6', '2.2', 'Iris-virginica'],\n",
       " ['6.3', '2.8', '5.1', '1.5', 'Iris-virginica'],\n",
       " ['6.1', '2.6', '5.6', '1.4', 'Iris-virginica'],\n",
       " ['7.7', '3.0', '6.1', '2.3', 'Iris-virginica'],\n",
       " ['6.3', '3.4', '5.6', '2.4', 'Iris-virginica'],\n",
       " ['6.4', '3.1', '5.5', '1.8', 'Iris-virginica'],\n",
       " ['6.0', '3.0', '4.8', '1.8', 'Iris-virginica'],\n",
       " ['6.9', '3.1', '5.4', '2.1', 'Iris-virginica'],\n",
       " ['6.7', '3.1', '5.6', '2.4', 'Iris-virginica'],\n",
       " ['6.9', '3.1', '5.1', '2.3', 'Iris-virginica'],\n",
       " ['5.8', '2.7', '5.1', '1.9', 'Iris-virginica'],\n",
       " ['6.8', '3.2', '5.9', '2.3', 'Iris-virginica'],\n",
       " ['6.7', '3.3', '5.7', '2.5', 'Iris-virginica'],\n",
       " ['6.7', '3.0', '5.2', '2.3', 'Iris-virginica'],\n",
       " ['6.3', '2.5', '5.0', '1.9', 'Iris-virginica'],\n",
       " ['6.5', '3.0', '5.2', '2.0', 'Iris-virginica'],\n",
       " ['6.2', '3.4', '5.4', '2.3', 'Iris-virginica'],\n",
       " ['5.9', '3.0', '5.1', '1.8', 'Iris-virginica'],\n",
       " []]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "raw,header = urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "with open(raw) as response:\n",
    "    data = list(csv.reader(response,delimiter=','))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster.k_means_:\n",
      "\n",
      "class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin)\n",
      " |  K-Means clustering\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, optional, default: 8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  max_iter : int, default: 300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  n_init : int, default: 10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  init : {'k-means++', 'random' or an ndarray}\n",
      " |      Method for initialization, defaults to 'k-means++':\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient by using the triangle\n",
      " |      inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
      " |      \"elkan\" for dense data and \"full\" for sparse data.\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances\n",
      " |  \n",
      " |      False : never precompute distances\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Relative tolerance with regards to inertia to declare convergence\n",
      " |  \n",
      " |  n_jobs : int\n",
      " |      The number of jobs to use for the computation. This works by computing\n",
      " |      each of the n_init runs in parallel.\n",
      " |  \n",
      " |      If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      " |      used at all, which is useful for debugging. For n_jobs below -1,\n",
      " |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      " |      are used.\n",
      " |  \n",
      " |  random_state : integer or numpy.RandomState, optional\n",
      " |      The generator used to initialize the centers. If an integer is\n",
      " |      given, it fixes the seed. Defaults to the global numpy random\n",
      " |      number generator.\n",
      " |  \n",
      " |  verbose : int, default 0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  copy_x : boolean, default True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first.  If copy_x is True, then the original data is not\n",
      " |      modified.  If False, the original data is modified, and put back before\n",
      " |      the function returns, but small numerical differences may be introduced\n",
      " |      by subtracting and then adding the data mean.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : array, [n_clusters, n_features]\n",
      " |      Coordinates of cluster centers\n",
      " |  \n",
      " |  labels_ :\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [4, 2], [4, 4], [4, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      " |  array([0, 1], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[ 1.,  2.],\n",
      " |         [ 4.,  2.]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The k-means problem is solved using Lloyd's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Training instances to cluster.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X, y=None)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cluster.KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class NearestNeighbors in module sklearn.neighbors.unsupervised:\n",
      "\n",
      "class NearestNeighbors(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.RadiusNeighborsMixin, sklearn.neighbors.base.UnsupervisedMixin)\n",
      " |  Unsupervised learner for implementing neighbor searches.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`k_neighbors` queries.\n",
      " |  \n",
      " |  radius : float, optional (default = 1.0)\n",
      " |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      " |      queries.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDtree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p: integer, optional (default = 2)\n",
      " |      Parameter for the Minkowski metric from\n",
      " |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : string or callable, default 'minkowski'\n",
      " |      metric to use for distance computation. Any metric from scikit-learn\n",
      " |      or scipy.spatial.distance can be used.\n",
      " |  \n",
      " |      If metric is a callable function, it is called on each\n",
      " |      pair of instances (rows) and the resulting value recorded. The callable\n",
      " |      should take two arrays as input and return one value indicating the\n",
      " |      distance between them. This works for Scipy's metrics, but is less\n",
      " |      efficient than passing the metric name as a string.\n",
      " |  \n",
      " |      Distance matrices are not supported.\n",
      " |  \n",
      " |      Valid values for metric are:\n",
      " |  \n",
      " |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      " |        'manhattan']\n",
      " |  \n",
      " |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      " |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      " |        'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
      " |        'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
      " |        'sqeuclidean', 'yule']\n",
      " |  \n",
      " |      See the documentation for scipy.spatial.distance for details on these\n",
      " |      metrics.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Affects only :meth:`k_neighbors` and :meth:`kneighbors_graph` methods.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |    >>> import numpy as np\n",
      " |    >>> from sklearn.neighbors import NearestNeighbors\n",
      " |    >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
      " |  \n",
      " |    >>> neigh = NearestNeighbors(2, 0.4)\n",
      " |    >>> neigh.fit(samples)  #doctest: +ELLIPSIS\n",
      " |    NearestNeighbors(...)\n",
      " |  \n",
      " |    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n",
      " |    ... #doctest: +ELLIPSIS\n",
      " |    array([[2, 0]]...)\n",
      " |  \n",
      " |    >>> nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)\n",
      " |    >>> np.asarray(nbrs[0][0])\n",
      " |    array(2)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  BallTree\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NearestNeighbors\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.RadiusNeighborsMixin\n",
      " |      sklearn.neighbors.base.UnsupervisedMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ....)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.RadiusNeighborsMixin:\n",
      " |  \n",
      " |  radius_neighbors(self, X=None, radius=None, return_distance=True)\n",
      " |      Finds the neighbors within a given radius of a point or points.\n",
      " |      \n",
      " |      Return the indices and distances of each point from the dataset\n",
      " |      lying in a ball with size ``radius`` around the points of the query\n",
      " |      array. Points lying on the boundary are included in the results.\n",
      " |      \n",
      " |      The result points are *not* necessarily sorted by distance to their\n",
      " |      query point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, (n_samples, n_features), optional\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      radius : float\n",
      " |          Limiting distance of neighbors to return.\n",
      " |          (default is the value passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array, shape (n_samples,) of arrays\n",
      " |          Array representing the distances to each point, only present if\n",
      " |          return_distance=True. The distance values are computed according\n",
      " |          to the ``metric`` constructor parameter.\n",
      " |      \n",
      " |      ind : array, shape (n_samples,) of arrays\n",
      " |          An array of arrays of indices of the approximate nearest points\n",
      " |          from the population matrix that lie within a ball of size\n",
      " |          ``radius`` around the query points.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1, 1, 1]:\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      " |      >>> print(np.asarray(rng[0][0])) # doctest: +ELLIPSIS\n",
      " |      [ 1.5  0.5]\n",
      " |      >>> print(np.asarray(rng[1][0])) # doctest: +ELLIPSIS\n",
      " |      [1 2]\n",
      " |      \n",
      " |      The first array returned contains the distances to all points which\n",
      " |      are closer than 1.6, while the second array returned contains their\n",
      " |      indices.  In general, multiple points can be queried at the same time.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Because the number of neighbors of each point is not necessarily\n",
      " |      equal, the results for multiple query points cannot be fit in a\n",
      " |      standard data array.\n",
      " |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      " |      each object is a 1D array of indices or distances.\n",
      " |  \n",
      " |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of Neighbors for points in X\n",
      " |      \n",
      " |      Neighborhoods are restricted the points at a distance lower than\n",
      " |      radius.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features], optional\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      radius : float\n",
      " |          Radius of neighborhoods.\n",
      " |          (default is the value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples]\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.radius_neighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  0.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      kneighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.UnsupervisedMixin:\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the model using X as training data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(NearestNeighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
